version: "3.5"

services:
  spider_posting:
    build: .
    image: clscraper
    depends_on:
      - pgdb
    env_file: .env
    environment:
      - http_proxy=$GATEWAY_IP:8081
      - https_proxy=$GATEWAY_IP:8081
    volumes:
      - .:/app
    command: bash -c "while :; do scrapy crawl postingspider; sleep 60; done"
    networks:
      - clscraper_default

  spider_list:
    build: .
    image: clscraper
    depends_on:
      - pgdb
    env_file: .env
    environment:
      - http_proxy=$GATEWAY_IP:8081
      - https_proxy=$GATEWAY_IP:8081
    volumes:
      - .:/app
    networks:
      - clscraper_default
    # command to scrape all listings
    # command: bash -c "while :; do scrapy crawl listspider -a number_of_pages_to_scrape=-1 -a listing_type=rea -a url=https://vancouver.craigslist.org/d/real-estate/search/rea; sleep 60; done"
    # command to scrape first page every 60 seconds 
    command: bash -c "while :; do scrapy crawl listspider -a listing_type=apa; sleep 60; done"

  pgdb:
    image: postgres:12.3
    volumes:
      - postgresdata:/var/lib/postgresql/data
    env_file: .env
    ports:
      - "54320:5432"
    networks:
      - clscraper_default

  testdb:
    image: postgres:12.3
    env_file: .env

volumes:
  postgresdata:

networks:
  clscraper_default:
      name: clscraper_default
      driver: bridge
